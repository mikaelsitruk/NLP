{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the article from https://www.kdnuggets.com/2020/01/intro-guide-nlp-data-scientists.html\n",
    "\n",
    "# 1. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the process of cutting text in sentences or words. This can be done with nltk, but first let's download what we need..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mikael\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'Mike', 'and', 'I', 'love', 'NLP', ',', 'I', 'live', 'in', 'New', 'Orleans', '.', 'New', 'York', 'is', 'a', 'beautiful', 'place', '!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sentence = \"My name is Mike and I love NLP, I live in New Orleans. New York is a beautiful place!\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This split the text into words, and ponctuations characters. \n",
    "Note: New York the city was \"understood\" as two words, there is not much of intelligence here...\n",
    "\n",
    "Interesting if we do it in French..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mon', 'nom', 'est', 'Mike', 'et', \"j'aime\", 'le', 'NLP', ',', \"j'habite\", 'a', 'Paris', 'mais', \"j'aime\", 'aussi', 'beaucoup', 'New-York', '!']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Mon nom est Mike et j'aime le NLP, j'habite a Paris mais j'aime aussi beaucoup New-York!\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting this time New-York was with a dash and was not split in two words, on the other hand \"j'aime\" should have been marked as two words but was seen as single one!\n",
    "\n",
    "__TODO: need to check how to process other than english texts...__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Stop word removal\n",
    "This step allows us to remove the noise from the text by removing the common words like \"the\", \"a\", \"and\"... to reduce the noise. Note that the punctuation is still present but \"is\", \"and\", \"in\",\"a\" were removed since defined as stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'Mike', 'I', 'love', 'NLP', ',', 'I', 'live', 'New', 'Orleans', '.', 'New', 'York', 'beautiful', 'place', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mikael\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sentence = \"My name is Mike and I love NLP, I live in New Orleans. New York is a beautiful place!\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "filtered_tokens = [w for w in tokens if w not in stop_words]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with the french sentence and a french dictionnary...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mon', 'nom', 'Mike', \"j'aime\", 'NLP', ',', \"j'habite\", 'a', 'Paris', \"j'aime\", 'aussi', 'beaucoup', 'New-York', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mikael\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sentence = \"Mon nom est Mike et j'aime le NLP, j'habite a Paris mais j'aime aussi beaucoup New-York!\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "stop_words = stopwords.words('french')\n",
    "filtered_tokens = [w for w in tokens if w not in stop_words]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So \"est\", \"et\", \"le\" , \"mais\" were removed, but was also expected to remove the \"a\" that indicate location and to have the \"j'\" as stand alone...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Stemming\n",
    "Stemming is the process of reducing words to their root. For example eat, eats, eating, ate are all from the eat root. Stemming simplifies the analysis of the text, since it reduce the number of word variations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat --> eat\n",
      "eats --> eat\n",
      "eating --> eat\n",
      "eaten --> eaten\n",
      "ate --> ate\n",
      "cook --> cook\n",
      "cooked --> cook\n",
      "cooking --> cook\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "snowball_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "words = (\"eat\" ,\"eats\", \"eating\", \"eaten\", \"ate\", \"cook\", \"cooked\", \"cooking\")\n",
    "\n",
    "for w in words:\n",
    "        print(f\"{w} --> {snowball_stemmer.stem(w)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the question: Why 'eaten' and 'ate' were not recognized? It seems that the problem is related to the fact that __stemming__ is based on an algorythm and therefore not all the form of the words are correctly captured.\n",
    "\n",
    "# 4. Word Embedding\n",
    "\n",
    "Word embedding allows us to move word representation to a format that is processable (e.g. a set of numerical values). But of course numberical values only are not enough (on ehot encoding is not really useful here for example) we need the representation to catch the similarity between words. So 2 words are similar if their numerical representation is close to each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
